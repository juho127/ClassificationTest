{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-10 Model Comparison: MLP vs CNN vs ViT\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juho127/ClassificationTest/blob/main/model_comparison.ipynb)\n",
        "\n",
        "This notebook compares three different architectures for CIFAR-10 classification:\n",
        "1. **MLP** (Multi-Layer Perceptron) - Basic feedforward network\n",
        "2. **CNN** (Convolutional Neural Network) - Spatial feature extraction\n",
        "3. **ViT** (Vision Transformer) - Attention-based architecture\n",
        "\n",
        "## Expected Performance\n",
        "- MLP: ~50-55%\n",
        "- CNN: ~70-75%\n",
        "- ViT: ~65-70%\n",
        "\n",
        "## Learning Goals\n",
        "- Understand the differences between architectures\n",
        "- See how inductive biases affect performance\n",
        "- Compare training time and model complexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running on Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"âœ“ Running on Google Colab\")\n",
        "    print(\"ðŸ“Œ Tip: Runtime > Change runtime type > GPU for faster training!\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"âœ“ Running on local environment\")\n",
        "\n",
        "# Install required packages on Colab\n",
        "if IN_COLAB:\n",
        "    print(\"\\nInstalling packages...\")\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install -q torch torchvision tqdm matplotlib einops\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"ðŸŽ‰ You can use GPU for faster training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hyperparameters and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 20  # Reduce to 10 for faster testing\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# CIFAR-10 classes\n",
        "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', \n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "if DEVICE.type == 'cuda':\n",
        "    print(\"âœ“ Using GPU!\")\n",
        "else:\n",
        "    print(\"â„¹ Using CPU (Colab: Runtime > Change runtime type > GPU)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing with augmentation for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "print(\"Loading dataset...\")\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images\n",
        "def show_images(loader, num_images=10):\n",
        "    dataiter = iter(loader)\n",
        "    images, labels = next(dataiter)\n",
        "    \n",
        "    # Denormalize\n",
        "    images = images / 2 + 0.5\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    fig.suptitle('CIFAR-10 Sample Images', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for idx, ax in enumerate(axes.flat):\n",
        "        if idx < num_images:\n",
        "            img = images[idx].numpy().transpose((1, 2, 0))\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f'{CLASSES[labels[idx]]}', fontsize=10)\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_images(train_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Definitions\n",
        "\n",
        "We'll define three different architectures and compare their performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 MLP Model (Multi-Layer Perceptron)\n",
        "\n",
        "Simple feedforward network that flattens the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi-Layer Perceptron\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size=3072, hidden_size1=512, hidden_size2=256, num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Flatten image (32x32x3 = 3072)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create MLP model\n",
        "mlp_model = MLP().to(DEVICE)\n",
        "print(mlp_model)\n",
        "print(f\"\\nMLP Parameters: {sum(p.numel() for p in mlp_model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"Convolutional Neural Network\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)  # 32 -> 16\n",
        "        x = self.conv2(x)  # 16 -> 8\n",
        "        x = self.conv3(x)  # 8 -> 4\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create CNN model\n",
        "cnn_model = CNN().to(DEVICE)\n",
        "print(cnn_model)\n",
        "print(f\"\\nCNN Parameters: {sum(p.numel() for p in cnn_model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 ViT Model (Vision Transformer)\n",
        "\n",
        "Uses self-attention mechanisms to process image patches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them\"\"\"\n",
        "    \n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e h w -> b (h w) e')\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer Encoder Block\"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        \n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Multi-head self-attention\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        # MLP\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Vision Transformer (Small size for CIFAR-10)\"\"\"\n",
        "    \n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "                 embed_dim=256, depth=6, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Patch embedding\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        \n",
        "        # Class token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        \n",
        "        # Positional embedding\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(dropout)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        \n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        # Classification head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        \n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        \n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "        \n",
        "        # Add class token\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=B)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        \n",
        "        # Add positional embedding\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.norm(x)\n",
        "        \n",
        "        # Classification token\n",
        "        cls_token_final = x[:, 0]\n",
        "        x = self.head(cls_token_final)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create ViT model\n",
        "vit_model = ViT().to(DEVICE)\n",
        "print(vit_model)\n",
        "print(f\"\\nViT Parameters: {sum(p.numel() for p in vit_model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model sizes\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"MLP Parameters: {count_parameters(mlp_model):,}\")\n",
        "print(f\"CNN Parameters: {count_parameters(cnn_model):,}\")\n",
        "print(f\"ViT Parameters: {count_parameters(vit_model):,}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, epoch, model_name):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'{model_name} Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{running_loss/total:.4f}',\n",
        "            'acc': f'{100*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    test_loss = test_loss / total\n",
        "    test_acc = 100 * correct / total\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "def train_model(model, model_name, num_epochs=NUM_EPOCHS):\n",
        "    \"\"\"Complete training loop for a model\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    test_losses = []\n",
        "    test_accs = []\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, epoch, model_name)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accs.append(test_acc)\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Test  - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n{model_name} Training Complete!\")\n",
        "    print(f\"Total training time: {training_time/60:.2f} minutes\")\n",
        "    print(f\"Best test accuracy: {max(test_accs):.2f}%\")\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'test_losses': test_losses,\n",
        "        'test_accs': test_accs,\n",
        "        'training_time': training_time,\n",
        "        'best_acc': max(test_accs)\n",
        "    }\n",
        "\n",
        "print(\"Training functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Models\n",
        "\n",
        "Now let's train each model and compare the results. Run the cells below one by one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Train MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MLP model\n",
        "mlp_results = train_model(mlp_model, \"MLP\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Train CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN model\n",
        "cnn_results = train_model(cnn_model, \"CNN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Train ViT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train ViT model\n",
        "vit_results = train_model(vit_model, \"ViT\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compare Results\n",
        "\n",
        "Now let's visualize and compare the performance of all three models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Summary Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary comparison\n",
        "results = {\n",
        "    'MLP': mlp_results,\n",
        "    'CNN': cnn_results,\n",
        "    'ViT': vit_results\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Model':<10} {'Parameters':<15} {'Best Acc':<12} {'Time (min)':<12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "models = [mlp_model, cnn_model, vit_model]\n",
        "names = ['MLP', 'CNN', 'ViT']\n",
        "\n",
        "for name, model in zip(names, models):\n",
        "    params = count_parameters(model)\n",
        "    best_acc = results[name]['best_acc']\n",
        "    time_taken = results[name]['training_time'] / 60\n",
        "    print(f\"{name:<10} {params:<15,} {best_acc:<12.2f}% {time_taken:<12.2f}\")\n",
        "\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Training Curves Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "epochs = range(1, NUM_EPOCHS + 1)\n",
        "\n",
        "# Training Loss\n",
        "ax = axes[0, 0]\n",
        "ax.plot(epochs, mlp_results['train_losses'], 'b-', label='MLP', linewidth=2)\n",
        "ax.plot(epochs, cnn_results['train_losses'], 'g-', label='CNN', linewidth=2)\n",
        "ax.plot(epochs, vit_results['train_losses'], 'r-', label='ViT', linewidth=2)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Test Loss\n",
        "ax = axes[0, 1]\n",
        "ax.plot(epochs, mlp_results['test_losses'], 'b-', label='MLP', linewidth=2)\n",
        "ax.plot(epochs, cnn_results['test_losses'], 'g-', label='CNN', linewidth=2)\n",
        "ax.plot(epochs, vit_results['test_losses'], 'r-', label='ViT', linewidth=2)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.set_title('Test Loss', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Training Accuracy\n",
        "ax = axes[1, 0]\n",
        "ax.plot(epochs, mlp_results['train_accs'], 'b-', label='MLP', linewidth=2)\n",
        "ax.plot(epochs, cnn_results['train_accs'], 'g-', label='CNN', linewidth=2)\n",
        "ax.plot(epochs, vit_results['train_accs'], 'r-', label='ViT', linewidth=2)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Test Accuracy\n",
        "ax = axes[1, 1]\n",
        "ax.plot(epochs, mlp_results['test_accs'], 'b-', label='MLP', linewidth=2)\n",
        "ax.plot(epochs, cnn_results['test_accs'], 'g-', label='CNN', linewidth=2)\n",
        "ax.plot(epochs, vit_results['test_accs'], 'r-', label='ViT', linewidth=2)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Test Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Comparison plot saved as 'model_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Bar Chart Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart for final metrics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "model_names = ['MLP', 'CNN', 'ViT']\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
        "\n",
        "# Best Accuracy\n",
        "ax = axes[0]\n",
        "accuracies = [mlp_results['best_acc'], cnn_results['best_acc'], vit_results['best_acc']]\n",
        "bars = ax.bar(model_names, accuracies, color=colors, alpha=0.8)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Best Test Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim([0, 100])\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{acc:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Training Time\n",
        "ax = axes[1]\n",
        "times = [mlp_results['training_time']/60, cnn_results['training_time']/60, vit_results['training_time']/60]\n",
        "bars = ax.bar(model_names, times, color=colors, alpha=0.8)\n",
        "ax.set_ylabel('Time (minutes)', fontsize=12)\n",
        "ax.set_title('Training Time', fontsize=14, fontweight='bold')\n",
        "for bar, time in zip(bars, times):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{time:.1f}m', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Parameters\n",
        "ax = axes[2]\n",
        "params = [count_parameters(mlp_model)/1e6, count_parameters(cnn_model)/1e6, count_parameters(vit_model)/1e6]\n",
        "bars = ax.bar(model_names, params, color=colors, alpha=0.8)\n",
        "ax.set_ylabel('Parameters (Millions)', fontsize=12)\n",
        "ax.set_title('Model Size', fontsize=14, fontweight='bold')\n",
        "for bar, param in zip(bars, params):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{param:.2f}M', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_metrics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Metrics comparison saved as 'model_metrics.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate per-class accuracy for each model\n",
        "def get_class_accuracy(model):\n",
        "    class_correct = [0] * 10\n",
        "    class_total = [0] * 10\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "    \n",
        "    class_acc = [100 * class_correct[i] / class_total[i] for i in range(10)]\n",
        "    return class_acc\n",
        "\n",
        "print(\"Calculating per-class accuracy...\")\n",
        "mlp_class_acc = get_class_accuracy(mlp_model)\n",
        "cnn_class_acc = get_class_accuracy(cnn_model)\n",
        "vit_class_acc = get_class_accuracy(vit_model)\n",
        "\n",
        "# Plot per-class accuracy\n",
        "x = np.arange(len(CLASSES))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "bars1 = ax.bar(x - width, mlp_class_acc, width, label='MLP', color='#3498db', alpha=0.8)\n",
        "bars2 = ax.bar(x, cnn_class_acc, width, label='CNN', color='#2ecc71', alpha=0.8)\n",
        "bars3 = ax.bar(x + width, vit_class_acc, width, label='ViT', color='#e74c3c', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Per-Class Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(CLASSES, fontsize=11)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "ax.set_ylim([0, 100])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('per_class_accuracy.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPer-class accuracy saved as 'per_class_accuracy.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print detailed per-class accuracy\n",
        "print(\"\\nDetailed Per-Class Accuracy:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Class':<12} {'MLP':<15} {'CNN':<15} {'ViT':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, cls in enumerate(CLASSES):\n",
        "    print(f\"{cls:<12} {mlp_class_acc[i]:<15.2f}% {cnn_class_acc[i]:<15.2f}% {vit_class_acc[i]:<15.2f}%\")\n",
        "\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Observations and Analysis\n",
        "\n",
        "### MLP (Multi-Layer Perceptron)\n",
        "**Pros:**\n",
        "- âœ“ Simple architecture, easy to understand\n",
        "- âœ“ Fast training\n",
        "- âœ“ Fewer parameters\n",
        "\n",
        "**Cons:**\n",
        "- âœ— Cannot capture spatial relationships in images\n",
        "- âœ— Treats pixels independently\n",
        "- âœ— Limited accuracy (~50-55%)\n",
        "\n",
        "**Best for:** Quick baseline, understanding basics\n",
        "\n",
        "---\n",
        "\n",
        "### CNN (Convolutional Neural Network)\n",
        "**Pros:**\n",
        "- âœ“ Best accuracy (~70-75%)\n",
        "- âœ“ Exploits spatial structure of images\n",
        "- âœ“ Parameter efficient (shared weights)\n",
        "- âœ“ Translation invariant\n",
        "\n",
        "**Cons:**\n",
        "- âœ— Fixed receptive field\n",
        "- âœ— Limited global context\n",
        "\n",
        "**Best for:** Image classification, spatial pattern recognition\n",
        "\n",
        "---\n",
        "\n",
        "### ViT (Vision Transformer)\n",
        "**Pros:**\n",
        "- âœ“ Global attention mechanism\n",
        "- âœ“ Flexible architecture\n",
        "- âœ“ Good performance (~65-70%)\n",
        "\n",
        "**Cons:**\n",
        "- âœ— Needs more data to excel\n",
        "- âœ— Slower training\n",
        "- âœ— More parameters\n",
        "\n",
        "**Best for:** Large datasets, transfer learning\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "For CIFAR-10:\n",
        "1. **CNN performs best** - Strong inductive biases for images\n",
        "2. **ViT is competitive** - But needs more data to excel (shines on ImageNet)\n",
        "3. **MLP is limited** - Cannot exploit spatial structure\n",
        "\n",
        "**Recommendation:** \n",
        "- Use **CNN** for small image datasets like CIFAR-10\n",
        "- Consider **ViT** for larger datasets (ImageNet) or with pre-training\n",
        "- Use **MLP** only for learning purposes or non-spatial data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Discussion Questions\n",
        "\n",
        "Think about and discuss:\n",
        "\n",
        "1. **Why does CNN outperform MLP on CIFAR-10?**\n",
        "   - Hint: Think about spatial structure and parameter sharing\n",
        "\n",
        "2. **Why doesn't ViT outperform CNN here?**\n",
        "   - Hint: Consider the dataset size and inductive biases\n",
        "\n",
        "3. **When would you choose each architecture?**\n",
        "   - MLP: ?\n",
        "   - CNN: ?\n",
        "   - ViT: ?\n",
        "\n",
        "4. **What happens if we train longer?**\n",
        "   - Will the gap between models increase or decrease?\n",
        "\n",
        "5. **Which classes are harder to classify? Why?**\n",
        "   - Look at the per-class accuracy chart\n",
        "\n",
        "6. **How can we improve each model?**\n",
        "   - MLP: ?\n",
        "   - CNN: ?\n",
        "   - ViT: ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Models (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models\n",
        "torch.save(mlp_model.state_dict(), 'mlp_model.pth')\n",
        "torch.save(cnn_model.state_dict(), 'cnn_model.pth')\n",
        "torch.save(vit_model.state_dict(), 'vit_model.pth')\n",
        "\n",
        "print(\"Models saved!\")\n",
        "print(\"  - mlp_model.pth\")\n",
        "print(\"  - cnn_model.pth\")\n",
        "print(\"  - vit_model.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
